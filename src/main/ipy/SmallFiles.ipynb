{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Storing Small Files with dCache"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This article describes a simple way to use <em>dCache</em> to efficiently store\n",
      "and retrieve small files on tertiary storage systems. To avoid the overhead of\n",
      "writing every file of a series by itself, we developed a set of scripts that\n",
      "will add a layer of abstraction to transparently bundle the files and store them\n",
      "together in an archive. This only requires minimal configuration on <em>dCache</em> \n",
      "and can be used with any dCache version 2.6.3 and higher."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Requirements"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The required configuration to use the scripts are a working <em>dCache</em> version 2.6.3 or higher installation. The pools that store the small files have to be configured with <pre>lfs=none</pre> and the machines running the pools need to be able to access the <em>Chimera</em> DB using <tt>psql</tt>. Their size should be big enough to store all the new files for at least one day. They must have the <tt>dcap</tt>\n",
      "libraries installed. \n",
      "\n",
      "The work station that packs the files needs to mount <em>Chimera</em> via <em>NFS4.1</em>."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Installation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All files needed can be obtained via the git repository https://github.com/kschwank/SmallFiles\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To install the Small Files capabilities do"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "In the namespace"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. create a directory to be used by the scripts (e.g. <tt>hsm</tt>)\n",
      "1. create two subdirectories to the <tt>hsm</tt>-directory called <tt>archives</tt> and <tt>requests</tt>\n",
      "1. on <tt>archives</tt> set <tt>AccessLatency</tt> to <tt>NEARLINE</tt> and <tt>RetentionPolicy</tt> to <tt>CUSTODIAL</tt>\n",
      "1. on <tt>requests</tt> set <tt>AccessLatency</tt> to <tt>ONLINE</tt> and <tt>RetentionPolicy</tt> to <tt>REPLICA</tt>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "On the NFS4.1 door"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. add the file producing node to the <tt>/etc/dcache/exports</tt> with <tt>rw,no_root_squash</tt> access rights\n",
      "1. add the packing node to the <tt>/etc/dcache/exports</tt> with <tt>rw,no_root_squash</tt> access rights\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "On the pool nodes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. install dcap libraries <pre>yum install dcap</pre>\n",
      "1. install the scala-library 2.10 by copying scala-library.jar to <tt>/usr/local/lib/</tt>\n",
      "1. install nailgun client by copying <tt>ng</tt> to <tt>/usr/local/bin/</tt>\n",
      "1. install nailgun server by copying <tt>nailgun-server-0.9.2-SNAPSHOT.jar</tt> to <tt>/usr/local/lib/</tt>\n",
      "1. install the ScalaChimeraNailgun library by copying it to <tt>/usr/local/lib/</tt>\n",
      "1. configure the symlinks to ng (e.g., <tt>ln -s /usr/local/bin/ng /usr/local/bin/cpathof</tt>)\n",
      "1. start the nailgun server (<tt>/usr/local/bin/runNailgunServer.sh</tt>)\n",
      "1. configure the aliases for the nailgun server (<tt>e.g., ng ng-alias cpathof</tt>, this is done by <tt>loadChimaNailgun.sh</tt>)\n",
      "1. edit <tt>hsm-internal.sh</tt> and set the following variables:\n",
      "    1. <tt>LOG</tt> - default: <tt>LOG=/tmp/hsmio.log</tt>\n",
      "    1. <tt>AWK</tt> - default: <tt>AWK=gawk</tt>\n",
      "    1. <tt>LIBPDCAP</tt> - default: <tt>LIBPDCAP=/usr/lib64/libpdcap.so.1</tt>\n",
      "    1. <tt>DCAP_DOOR</tt> - default: <tt>DCAP_DOOR=\"localhost:22125\"</tt>\n",
      "    1. <tt>CHIMERA_PARAMS</tt> - default: <tt>CHIMERA_PARAMS=\"org.postgresql.Driver jdbc:postgresql://dcache-lab000/chimera?prepareThreshold=3 PgSQL chimera - \"</tt>\n",
      "1. set max storage handlers to 2 or higher (max 10) (<tt>st set max active 2</tt> in admin interface)\n",
      "1. set max restore handlers to 2 or higher (<tt>rh set max active 2</tt> in admin interface)\n",
      "1. configure the HSM <tt>dcache</tt> for your site on the pool in the admin interface\n",
      "    1. <code>hsm set dcache -hsmBase=hsm</code>\n",
      "    1. <code>hsm set dcache -command=/usr/share/dcache/lib/hsm-internal-[by-dir|by-group].sh</code>\n",
      "    1. <code>hsm set dcache -dataRoot=/data</code>\n",
      "    1. <code>save</code>\n",
      "1. configure the HSM for the archive files\n",
      "\n",
      "Finally the PoolManager has to be configured to allow staging and pool to pool transfers:\n",
      "\n",
      "1. <code>cd PoolManager</code>\n",
      "1. <code>pm set -p2p-allowed=yes</code>\n",
      "1. <code>pm set -p2p-fortransfers=yes</code>\n",
      "1. <code>pm set -stage-allowed=yes</code>\n",
      "1. <code>save</code>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This is not yet integrated, but might be a good idea\n",
      "# 1. <code>hsm set dcache chimeraArgs=\"org.postgresql.Driver jdbc:postgresql://dcache-lab000/chimera?prepareThreshold=3 PgSQL chimera - \"</code>"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Note:** <tt>ng</tt> is a native binary and might therefore need to be rebuild for the target platform. To do that, clone the nailgun repository from https://github.com/martylamb/nailgun.git and run <pre>make ng</pre>."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Important: Before any request is made to the nailgun server, make sure it is fully started! After running run-ng-server, wait at least one minute before starting the pool.**"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "On the file producing note"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. mount the NFS namespace to a local directory (e.g. <tt>/pnfs/4</tt>)\n",
      "1. on the directories used to store the small files set the tags\n",
      "    1. <tt>hsmInstance</tt> to <tt>dcache</tt>\n",
      "    1. <tt>OSMTemplate</tt> to <tt>StoreName [data]</tt>, arbitrary but neccessary, e.g., <tt>archived</tt>\n",
      "    1. <tt>sGroup</tt> to <tt>[sGroup]</tt>, arbitrary but neccessary, e.g., <tt>smallfiles</tt>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "On the packing node"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. mount the NFS namespace to a local directory (e.g. <tt>/pnfs/4</tt>)\n",
      "1. install the packing script (<tt>pack-files.sh</tt>) into your path\n",
      "1. install a cron job that calls the packing script. E.g., \n",
      "\n",
      "<code>\\*  \\*  \\*  \\*  \\* root /usr/local/bin/pack-files.sh <em>dataRoot</em> <em>nfsMountpoint</em> <em>hsmSubdir</em> <em>archiveSize</em> [<em>packRemainingInterval</em>] 2>&1</code>\n",
      "\n",
      "where\n",
      "\n",
      "- <em>dataRoot</em> is the pnfs path prefix. E.g., <tt>/data</tt> or <tt>/exports/data</tt>\n",
      "- <em>pnfsMountpoint</em> is the local mountpoint of the namespace. E.g., <tt>/pnfs/4</tt>\n",
      "- <em>hsmSubdir</em> is the directory name containing the <tt>archives</tt> and <tt>requests</tt> directories (s.a.). E.g., <tt>hsm</tt>\n",
      "- <em>archiveSize</em> is the approximate size of the archives in bytes. Set to 0 to pack as many as possible.\n",
      "- <em>packRemainingInterval</em> is a time period in seconds after that, if no new files were added to the directory, the remaining files are packed into an extra archive."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Implementation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The implementation in its current form consists of two scripts, namely <tt>hsm-internal.sh</tt> and <tt>pack-files.sh</tt>. The first script is called for every file and creates a flag that is later used by the second script to bundle the files together. Only files residing in the same (sub-)directory will be bundled together.\n",
      "\n",
      "After configuration the <em>dCache</em> pools call <tt>hsm-internal.sh</tt> for \n",
      "every file of <em>OSM</em> type <tt>dcache</tt>. The <tt>hsm-internal</tt> script\n",
      "has two commands: <tt>put</tt> and <tt>get</tt>. As soon as a new file for OSM\n",
      "type <tt>dcache</tt> arrives at a pool, the script is called with the command \n",
      "<tt>put</tt>. This checks if a flag file for this file does already exist. If it\n",
      "does the script checks if the file associated with the flag is already archived.\n",
      "If that is the case the flag file is deleted. If the flag file does not yet exist\n",
      "it is created in a per subdirectory unique subdirectory below <tt>hsm/reqests</tt>. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image\n",
      "Image(filename='../resources/img/Whiteboard-SF-put.png', width=1024)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cd ../msc\n",
      "mscgen -T svg -o put.svg -i put.msc\n",
      "mscgen -T svg -o check.svg -i check.msc\n",
      "mscgen -T svg -o clear.svg -i clear.msc\n",
      "mscgen -T svg -o pack.svg -i pack.msc\n",
      "mscgen -T svg -o get.svg -i get.msc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When a fresh file arrives from the source the pool calls hsm-internal.sh which will create a flag file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import SVG\n",
      "SVG(filename='../msc/put.svg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The file is then marked for a retry. Let's assume the packing script did not archive the file when the flush retry interval expires the next time. Then the retry is exactly like the first, except that this time the flag file already exists:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SVG(filename='../msc/check.svg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The file is again marked for retry. Let's now assume that the packing script ran and collected the file. Then the retry will look like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SVG(filename='../msc/clear.svg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Packing the files wirks like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SVG(filename='../msc/pack.svg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To get a previously archived file that was already removed from the pool the <tt>hsm-internal.sh</tt> script is called with the command <tt>get</tt>:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SVG(filename='../msc/get.svg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The statistics were created using the following machines:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- <tt>dcache-lab000</tt> as <em>dCache</em> head node with <em>NFS4.1</em> door\n",
      "- <tt>dcache-lab001</tt> as pool node with <tt>st set max active 2</tt> (only in multiple pools runs)\n",
      "- <tt>dcache-lab002</tt> as pool node with <tt>st set max active 2</tt>\n",
      "- <tt>christian-vm01</tt> to inject small files and to periodically (1/min) run the <tt>pack-files.sh</tt>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The statistics are displayed using numpy, pandas and matplotlib:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The files containing the statistics are included in the repository. We load them all here and combine them into one big <em>DataFrame</em> called <tt>allres</tt> for analysis. Note: We also add two more columns called <tt>fps</tt> for files per second and <tt>bps</tt> for bytes per second."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "create1res = pd.io.parsers.read_csv('../resources/csv/create.csv', header=0)\n",
      "create2res = pd.io.parsers.read_csv('../resources/csv/create2.csv', header=0)\n",
      "createres = create2res\n",
      "findres = pd.io.parsers.read_csv('../resources/csv/find2.csv',header=0)\n",
      "lsres = pd.io.parsers.read_csv('../resources/csv/ls2.csv', header=0)\n",
      "statres = pd.io.parsers.read_csv('../resources/csv/stat2.csv', header=0)\n",
      "writeres = pd.io.parsers.read_csv('../resources/csv/write2.csv', header=0)\n",
      "readres = pd.io.parsers.read_csv('../resources/csv/read2.csv', header=0)\n",
      "pathofres = pd.io.parsers.read_csv('../resources/csv/pathof.csv', header=0)                              \n",
      "tarres = pd.io.parsers.read_csv('../resources/csv/tar8.csv', header=0)\n",
      "zipres = pd.io.parsers.read_csv('../resources/csv/zip8.csv', header=0)\n",
      "\n",
      "results = { 'create' : createres, \n",
      "           'find' : findres, \n",
      "           'ls' : lsres, \n",
      "           'stat' : statres, \n",
      "           'write' : writeres, \n",
      "           'read' : readres, \n",
      "           'pathof' : pathofres, \n",
      "           'tar' : tarres, \n",
      "           'zip' : zipres }\n",
      "\n",
      "for op in results.keys():\n",
      "    df = results[op]\n",
      "    df['op'] = op\n",
      "    df['fps'] = df['file_count'] / df['total']\n",
      "    df['bps'] = df['fps'] * df['file_size']\n",
      "    results[op] = df\n",
      "    \n",
      "allres = pd.concat(results.values())\n",
      "allres.describe"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "allres.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Basic filesystem operations "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Copying files into <em>dCache</em>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "createres = allres[allres.op == 'create']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The file rate is mostly independent of the number of files written into on directory as can be seen on the linear graphs:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots()\n",
      "\n",
      "for key, grp in createres.groupby(['file_size']):\n",
      "    grp[['fps']].plot(ax=ax, label=str(key), x=grp.file_count, ylim=[0,20])\n",
      "    \n",
      "plt.legend(('1k','2k','4k','8k','40k','1M','2M','4M','8M'), loc='best')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But highly depends on the size of the files:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots()\n",
      "\n",
      "for key, grp in createres.groupby(['file_count']):\n",
      "    grp[['fps']].plot(ax=ax, label=str(key), x=grp.file_size, ylim=[0,20], rot=45)\n",
      "    \n",
      "plt.legend(('1000','2000','4000','10000','20000','100000'), loc='best')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "createres"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The performance decreases with the amount of files written in sequence. The following graphs show two measurements, the first writing 3000 5MB files each into 9 directories. The second shows writing 7000 5MB files each into 5 directories."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "storeperf = pd.io.parsers.read_csv('../resources/csv/storeperf.csv', header=0)\n",
      "storeperf['fps'] = 3000 / storeperf.total\n",
      "storeperf[['fps']].plot(x=storeperf.file_count, xlim=(0,40000), ylim=(0,8))\n",
      "storeperf2 = pd.io.parsers.read_csv('../resources/csv/storeperf2.csv', header=0)\n",
      "storeperf2['fps'] = 7000 / storeperf2.total\n",
      "storeperf2[['fps']].plot(x=storeperf2.file_count, xlim=(0,40000), ylim=(0,8))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Getting directory contents usind ls and find"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsres = allres[allres.op == 'ls']\n",
      "findres = allres[allres.op == 'find']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While the performance of <tt>ls</tt> and <tt>find</tt> does not differ significantly, the duration of both depends on the number of files in the directory:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots()\n",
      "\n",
      "ls2M=lsres[lsres.file_size == 2000000]\n",
      "find2M=findres[lsres.file_size == 2000000]\n",
      "ls2M[['fps']].plot(ax=ax, label='ls', x=ls2M.file_count)\n",
      "find2M[['fps']].plot(ax=ax, label='find', x=find2M.file_count)\n",
      "plt.legend(('ls', 'find'), loc = 'best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsres"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "findres"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "stat on files"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<tt>stat</tt> depends on the number of files in the directory, but, of course, not on the size of the files:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "statres = allres[allres.op == 'stat']\n",
      "#statres8M=statres[statres.file_size == 8000000]\n",
      "#statres8M[['fps']].plot(x=statres8M.file_count)\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "\n",
      "for key, grp in statres.groupby(['file_size']):\n",
      "    grp[['fps']].plot(ax=ax, label=str(key), x=grp.file_count)\n",
      "    \n",
      "plt.legend(('1k','2k','4k','8k','40k','1M','2M','4M','8M'), loc='best')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "statres"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Writing into file's metadata level"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "writeres = allres[allres.op == 'write']\n",
      "writeres1M=writeres[writeres.file_size == 1000000]\n",
      "writeres1M[['fps']].plot(x=writeres1M.file_count, ylim=(0,80))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "writeres"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Read the metadata level of all files in a directory"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "readres = allres[allres.op == 'read']\n",
      "readres1M=readres[readres.file_size == 1000000]\n",
      "readres1M[['fps']].plot(x=readres1M.file_count, ylim=(0,80))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "readres"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Querying the path by pnfsid using 'pathof'"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Getting the path information of a file using \"pathof\" is independent of the file size, but it does depend on the number of files in the directory:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pathofres = allres[allres.op == 'pathof']\n",
      "pathofres[['fps']].plot(x=pathofres.file_count, kind='bar')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pathofres"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Packing files"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The 'fps' rate of zip appears to be slightly better, especially for smaller files."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, axes = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
      "\n",
      "for key, grp in tarres.groupby(['file_size']):\n",
      "    grp[['fps']].plot(ax=axes[0], x=grp.file_count, rot=45, logx=True, legend=False)\n",
      "    \n",
      "for key, grp in zipres.groupby(['file_size']):\n",
      "    grp[['fps']].plot(ax=axes[1], x=grp.file_count, label=str(key), rot=45, logx=True, legend=False)\n",
      "\n",
      "ax0=axes[0]\n",
      "ax0.set_title('tar')\n",
      "ax1=axes[1]\n",
      "ax1.set_title('zip')\n",
      "    \n",
      "plt.legend(('1k', '40k', '1M', '8M'), loc='best')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tarres"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "zipres"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "System performance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following tables show durations for different configurations of file sizes, number of files and directories:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "systemres = pd.io.parsers.read_csv('../resources/csv/system.csv', header=0)\n",
      "systemres"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "systemres5M = systemres[(systemres.file_size == 5000000)]\n",
      "systemres5M['fps'] = systemres5M['file_count'] / systemres5M['create']\n",
      "systemres5M[['fps']].plot(kind='bar', x=systemres5M.dir_count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "systemres5M"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Optimizing parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. The more subdirectories are used, the more parallel the archivation can be.\n",
      "1. The more pools are involved, the faster can the flag files be created and removed - provided the DB is not the bottleneck.\n",
      "1. The interval of running the packing script (<tt>pack-files.sh</tt>) should match the time it takes to create as many flags as files are     expected to go into one archive.\n",
      "1. The size of the archives should be chosen so that the time for creating an archive matches the run interval of the packing script."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Problems and their effects"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. No space left on device <br>If the filesystem containing the log on a pool runs out of space, the <tt>hsm-internal.sh</tt> script will fail and cause the flagfiles not to be created or removed.\n",
      "1. Operations in the NFS4.1 mount hang <br> If the log of the nfs door shows: <tt>FATAL: remaining connection slots are reserved for non-replication superuser connections</tt>, it means that dCache has used up all free DB connection slots. This will cause any file operation on the NFS mount to fail and thus make the packing stop working. It might be necessary to restart the nailgun server."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Possible enhancements"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Chimera parameters on hsm level instead of hardcoded in the script\n",
      "1. Archive size per directory\n",
      "1. Unpack all files from archive instead of only requested\n",
      "1. Archive by file count or specify file size to improve performance\n",
      "1. Specify min and max size of archive\n",
      "1. Recovery: Check if the pid of the lockfile still exists and if it doesn't clear the lock\n",
      "1. Verify archive integrity further by using checksums or <tt>tar -d[hf] arc.tar /tmp/links/*</tt>\n",
      "    1. checksums, sums of checksum(?)\n",
      "    1. <tt>tar cWhf arc.tar /tmp/links/*</tt> - this takes much longer (factor 3 on a desktop machine)\n",
      "    1. <tt>tar -dhf arc.tar /tmp/links/*</tt> - takes about the same time as <tt>W</tt>\n",
      "1. Have different scripts with different parameters for different directories"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Scripts"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "speedtest.sh"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Used to generate the statistics concerning simple filesystem operations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat ../../skel/usr/local/bin/speedtest.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "hsm-internal-*.sh"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<tt>hsm-internal-*.sh</tt> is being called for each file of with <tt>osm=dcache</tt> with the command <tt>put</tt>. It is also used with the command <tt>get</tt> to retrieve the file again from the archive. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "hsm-internal-by-dir.sh"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat ../../skel/usr/share/dcache/lib/hsm-internal-by-dir.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "hsm-internal-by-group.sh"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Alternatively to packing by (sub-)directory the script <tt>hsm-internal-by-group.sh</tt> creates a slighly different directory structure that is used by <tt>pack-files-by-group.sh</tt> to pack the files solely by their <tt>OSMTemplate</tt> and <tt>sGroup</tt>."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!diff ../../skel/usr/share/dcache/lib/hsm-internal-by-dir.sh ../../skel/usr/share/dcache/lib/hsm-internal-by-group.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "pack-files-*.sh"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The <tt>pack-files-\\*.sh</tt> scripts collect the files created by their <tt>hsm-internal-\\*.sh</tt> counter parts and bundle them into archives."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "pack-files-by-dir.sh"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat ../../skel/usr/local/bin/pack-files-by-dir.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "pack-files-by-group.sh"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<tt>pack-files-by-group.sh</tt> is the counterpart to <tt>hsm-internal-by-group.sh</tt>. It only differs from the other packing script in the way it collects the directories containing the flag files."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!diff ../../skel/usr/local/bin/pack-files-by-dir.sh ../../skel/usr/local/bin/pack-files-by-group.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "SmallFilesTests.sh"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Used to test the whole small files system. The <tt>SmallFilesTests.sh</tt> script first cleans the test directories, then creates the directory structure that will hold the small files. The directory structure consists of a number of main directories, given by the value of variable <tt>dirCount</tt>. Under each of these directories will be created a number of subdirectories, given by the value of the variable <tt>subDirCount</tt>. As small files the script uses a template file given by the value of variable <tt>testFile</tt>.\n",
      "The number of files per directory (and subdirectory) is given by the variable <tt>filesPerDir</tt> and the number of files per archive if given by the variable <tt>filesPerArchive</tt>. The target archive size is then automatically infered from these numbers and a specialized entry is added to <tt>/etc/crontab</tt>. Then the creation of the files is started using the script <tt>pushFiles.sh</tt> that needs to be in the PATH."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat ../../skel/usr/local/bin/SmallFilesTest.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "pushFiles.sh"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Used to create lots of files in a directory."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat ../../skel/usr/local/bin/pushFiles.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "check-archives.sh"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Used to manually verify the integrity of the archives generated by the <tt>pack-files.sh</tt> script. It has three commands: \n",
      "<pre>\n",
      "acount: count the archives in the given subdirectory\n",
      "fcount: count the files inside the archives\n",
      "dups:   cross-check for duplicate files in all archives"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat ../../skel/usr/local/bin/check-archives.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Nailgun related scripts"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "createNGlinks.sh"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This will generate the symlinks named after the commands used by the <tt>hsm-internal.sh</tt> script. It needs two parameters: <tt>bin path</tt> and <tt>ng path</tt>. <tt>bin path</tt> is the directory where the symlinks should be created, <tt>ng path</tt> is the path to the <tt>ng</tt> binary. If <tt>ng</tt> is on the PATH, <tt>ng path</tt> may be omitted."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat ../../skel/usr/local/bin/createNGlinks.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "run-ng-server"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This will start the Nailgun Server with the needed configuration. After the server is started the script sets the <em>classpath</em> to the necessary libraries (i.e., the dCache jars, the scala library and the ScalaChimeraNailgun classes). These may have to be adjusted."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat ../../skel/usr/local/bin/run-ng-server"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "loadChimeraNailgun.sh"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "loadChimeraNailgun.sh installs the aliases of the nailgun commands"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat ../../skel/usr/local/bin/loadChimeraNailgun.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}