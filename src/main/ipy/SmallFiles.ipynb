{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Storing Small Files with dCache"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This article describes a simple way to use <em>dCache</em> to efficiently store\n",
      "and retrieve small files on tertiary storage systems. To avoid the overhead of\n",
      "writing every file of a series by itself, we developed a set of scripts that\n",
      "will add a layer of abstraction to transparently bundle the files and store them\n",
      "together in an archive. This only requires minimal configuration on <em>dCache</em> \n",
      "and can be used with any dCache version 2.6 and higher."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Requirements"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The required configuration to use the scripts are a working <em>dCache</em> version 2.6.3 or higher installation. The pools should be big enough to store all the new files for at least one day and they must have the <tt>dcap</tt> libraries installed. \n",
      "\n",
      "The work station that packs the files needs to mount <em>Chimera</em> via <em>NFS4.1</em>."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Installation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All files needed can be obtained via the git repository https://github.com/kschwank/SmallFiles\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To install the Small Files capabilities do"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "In the namespace"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. create subdirectories to store the archives, e.g, \"archives\".\n",
      "1. on <tt>archives</tt> set <tt>AccessLatency</tt> to <tt>NEARLINE</tt> and <tt>RetentionPolicy</tt> to <tt>CUSTODIAL</tt>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "On the NFS4.1 door"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. add the file producing node to the <tt>/etc/dcache/exports</tt> with <tt>rw,no_root_squash</tt> access rights\n",
      "1. add the packing node to the <tt>/etc/dcache/exports</tt> with <tt>rw,no_root_squash</tt> access rights\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "On the pool nodes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. install dcap libraries <pre>yum install dcap</pre>\n",
      "1. edit <tt>hsm-internal.sh</tt> and set the following variables:\n",
      "    1. <tt>LOG</tt> - default: <tt>LOG=/tmp/hsmio.log</tt>\n",
      "    1. <tt>AWK</tt> - default: <tt>AWK=gawk</tt>\n",
      "    1. <tt>LIBPDCAP</tt> - default: <tt>LIBPDCAP=/usr/lib64/libpdcap.so.1</tt>\n",
      "    1. <tt>DCAP_DOOR</tt> - default: <tt>DCAP_DOOR=\"dcapdoor:22125\"</tt>\n",
      "1. set max storage handlers to 2 or higher (max 10) (<tt>st set max active 2</tt> in admin interface)\n",
      "1. set max restore handlers to 2 or higher (<tt>rh set max active 2</tt> in admin interface)\n",
      "1. configure the HSM <tt>dcache</tt> for your site on the pool in the admin interface\n",
      "    1. <code>hsm set dcache -command=/usr/share/dcache/lib/hsm-internal.sh</code>\n",
      "    1. <code>hsm set dcache -mongoUrl=mongohost/smallfiles</code>\n",
      "    1. <code>save</code>\n",
      "1. configure the HSM for the archive files\n",
      "\n",
      "Finally the PoolManager has to be configured to allow staging and pool to pool transfers:\n",
      "\n",
      "1. <code>cd PoolManager</code>\n",
      "1. <code>pm set -p2p-allowed=yes</code>\n",
      "1. <code>pm set -p2p-fortransfers=yes</code>\n",
      "1. <code>pm set -stage-allowed=yes</code>\n",
      "1. <code>save</code>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "On the file producing note"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. mount the NFS namespace to a local directory (e.g. <tt>/pnfs/4</tt>)\n",
      "1. on the directories used to store the small files set the tags\n",
      "    1. <tt>hsmInstance</tt> to <tt>dcache</tt>\n",
      "    1. <tt>OSMTemplate</tt> to <tt>StoreName [data]</tt>, arbitrary but neccessary, e.g., <tt>archived</tt>\n",
      "    1. <tt>sGroup</tt> to <tt>[sGroup]</tt>, arbitrary but neccessary, e.g., <tt>smallfiles</tt>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "On the packing node"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. mount the NFS namespace to a local directory (e.g. <tt>/pnfs/4</tt>)\n",
      "1. install the packing scripts (<tt>fillmetadata.py, pack-files.py, writebfids.py</tt>) into your path\n",
      "\n",
      "and make sure they are started automatically."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Implementation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The implementation in its current form consists of 4 scripts, namely <tt>hsm-internal.sh</tt>, <tt>pack-files.py</tt>, <tt>fillmetadata.py</tt> and <tt>writebfids.py</tt>. The first script is called for every file and creates a minimal entry in a Mongo database. This entry is then used by the <tt>fillmetadata.py</tt> script to request more metadata for that file from chimera using the dot-files on the NFS4.1 mount. These enriched entries are then used by the <tt>pack-files.py</tt> script to create archives. For every archive another entry is created in a different collection that then used by the <tt>writebfids.py</tt> script to add the <tt>archiveUrl</tt> to the file entries. These are periodically checked by the hsm-scripts and removed after the <tt>archiveUrl</tt> is extracted to be written into chimera."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cd ../msc\n",
      "mscgen -T svg -o put.svg -i put.msc\n",
      "mscgen -T svg -o check.svg -i check.msc\n",
      "mscgen -T svg -o clear.svg -i clear.msc\n",
      "mscgen -T svg -o pack.svg -i pack.msc\n",
      "mscgen -T svg -o get.svg -i get.msc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When a fresh file arrives from the source the pool calls hsm-internal.sh which will create an entry in the Mongo database:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import SVG\n",
      "SVG(filename='../msc/put.svg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The file is then marked for a retry. Let's assume the packing script did not archive the file when the flush retry interval expires the next time. Then the retry is exactly like the first, except that this time the flag file already exists:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SVG(filename='../msc/check.svg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The file is again marked for retry. Let's now assume that the packing script ran and collected the file. Then the retry will look like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SVG(filename='../msc/clear.svg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Packing the files works like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SVG(filename='../msc/pack.svg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To get a previously archived file that was already removed from the pool the <tt>hsm-internal.sh</tt> script is called with the command <tt>get</tt>:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SVG(filename='../msc/get.svg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The statistics were created using the following machines:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- <tt>dcache-lab000</tt> as <em>dCache</em> head node with <em>NFS4.1</em> door\n",
      "- <tt>dcache-lab001</tt> as pool node with <tt>st set max active 2</tt> (only in multiple pools runs)\n",
      "- <tt>dcache-lab002</tt> as pool node with <tt>st set max active 2</tt>\n",
      "- <tt>christian-vm01</tt> to inject small files and to periodically (1/min) run the <tt>pack-files.sh</tt>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The statistics are displayed using numpy, pandas and matplotlib:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The files containing the statistics are included in the repository. We load them all here and combine them into one big <em>DataFrame</em> called <tt>allres</tt> for analysis. Note: We also add two more columns called <tt>fps</tt> for files per second and <tt>bps</tt> for bytes per second."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "create1res = pd.io.parsers.read_csv('../resources/csv/create.csv', header=0)\n",
      "create2res = pd.io.parsers.read_csv('../resources/csv/create2.csv', header=0)\n",
      "createres = create2res\n",
      "findres = pd.io.parsers.read_csv('../resources/csv/find2.csv',header=0)\n",
      "lsres = pd.io.parsers.read_csv('../resources/csv/ls2.csv', header=0)\n",
      "statres = pd.io.parsers.read_csv('../resources/csv/stat2.csv', header=0)\n",
      "writeres = pd.io.parsers.read_csv('../resources/csv/write2.csv', header=0)\n",
      "readres = pd.io.parsers.read_csv('../resources/csv/read2.csv', header=0)\n",
      "pathofres = pd.io.parsers.read_csv('../resources/csv/pathof.csv', header=0)                              \n",
      "tarres = pd.io.parsers.read_csv('../resources/csv/tar8.csv', header=0)\n",
      "zipres = pd.io.parsers.read_csv('../resources/csv/zip8.csv', header=0)\n",
      "\n",
      "results = { 'create' : createres, \n",
      "           'find' : findres, \n",
      "           'ls' : lsres, \n",
      "           'stat' : statres, \n",
      "           'write' : writeres, \n",
      "           'read' : readres, \n",
      "           'pathof' : pathofres, \n",
      "           'tar' : tarres, \n",
      "           'zip' : zipres }\n",
      "\n",
      "for op in results.keys():\n",
      "    df = results[op]\n",
      "    df['op'] = op\n",
      "    df['fps'] = df['file_count'] / df['total']\n",
      "    df['bps'] = df['fps'] * df['file_size']\n",
      "    results[op] = df\n",
      "    \n",
      "allres = pd.concat(results.values())\n",
      "allres.describe"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "allres.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Basic filesystem operations "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Copying files into <em>dCache</em>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "createres = allres[allres.op == 'create']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The file rate is mostly independent of the number of files written into on directory as can be seen on the linear graphs:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots()\n",
      "\n",
      "for key, grp in createres.groupby(['file_size']):\n",
      "    grp[['fps']].plot(ax=ax, label=str(key), x=grp.file_count, ylim=[0,20])\n",
      "    \n",
      "plt.legend(('1k','2k','4k','8k','40k','1M','2M','4M','8M'), loc='best')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But highly depends on the size of the files:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots()\n",
      "\n",
      "for key, grp in createres.groupby(['file_count']):\n",
      "    grp[['fps']].plot(ax=ax, label=str(key), x=grp.file_size, ylim=[0,20], rot=45)\n",
      "    \n",
      "plt.legend(('1000','2000','4000','10000','20000','100000'), loc='best')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "createres"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The performance decreases with the amount of files written in sequence. The following graphs show two measurements, the first writing 3000 5MB files each into 9 directories. The second shows writing 7000 5MB files each into 5 directories."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "storeperf = pd.io.parsers.read_csv('../resources/csv/storeperf.csv', header=0)\n",
      "storeperf['fps'] = 3000 / storeperf.total\n",
      "storeperf[['fps']].plot(x=storeperf.file_count, xlim=(0,40000), ylim=(0,8))\n",
      "storeperf2 = pd.io.parsers.read_csv('../resources/csv/storeperf2.csv', header=0)\n",
      "storeperf2['fps'] = 7000 / storeperf2.total\n",
      "storeperf2[['fps']].plot(x=storeperf2.file_count, xlim=(0,40000), ylim=(0,8))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Getting directory contents usind ls and find"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsres = allres[allres.op == 'ls']\n",
      "findres = allres[allres.op == 'find']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While the performance of <tt>ls</tt> and <tt>find</tt> does not differ significantly, the duration of both depends on the number of files in the directory:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots()\n",
      "\n",
      "ls2M=lsres[lsres.file_size == 2000000]\n",
      "find2M=findres[lsres.file_size == 2000000]\n",
      "ls2M[['fps']].plot(ax=ax, label='ls', x=ls2M.file_count)\n",
      "find2M[['fps']].plot(ax=ax, label='find', x=find2M.file_count)\n",
      "plt.legend(('ls', 'find'), loc = 'best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsres"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "findres"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "stat on files"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<tt>stat</tt> depends on the number of files in the directory, but, of course, not on the size of the files:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "statres = allres[allres.op == 'stat']\n",
      "#statres8M=statres[statres.file_size == 8000000]\n",
      "#statres8M[['fps']].plot(x=statres8M.file_count)\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "\n",
      "for key, grp in statres.groupby(['file_size']):\n",
      "    grp[['fps']].plot(ax=ax, label=str(key), x=grp.file_count)\n",
      "    \n",
      "plt.legend(('1k','2k','4k','8k','40k','1M','2M','4M','8M'), loc='best')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "statres"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Writing into file's metadata level"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "writeres = allres[allres.op == 'write']\n",
      "writeres1M=writeres[writeres.file_size == 1000000]\n",
      "writeres1M[['fps']].plot(x=writeres1M.file_count, ylim=(0,80))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "writeres"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Read the metadata level of all files in a directory"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "readres = allres[allres.op == 'read']\n",
      "readres1M=readres[readres.file_size == 1000000]\n",
      "readres1M[['fps']].plot(x=readres1M.file_count, ylim=(0,80))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "readres"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Querying the path by pnfsid using 'pathof'"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Getting the path information of a file using \"pathof\" is independent of the file size, but it does depend on the number of files in the directory:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pathofres = allres[allres.op == 'pathof']\n",
      "pathofres[['fps']].plot(x=pathofres.file_count, kind='bar')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pathofres"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "System performance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following tables show durations for different configurations of file sizes, number of files and directories:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "systemres = pd.io.parsers.read_csv('../resources/csv/system.csv', header=0)\n",
      "systemres"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "systemres5M = systemres[(systemres.file_size == 5000000)]\n",
      "systemres5M['fps'] = systemres5M['file_count'] / systemres5M['create']\n",
      "systemres5M[['fps']].plot(kind='bar', x=systemres5M.dir_count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "systemres5M"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Scripts"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "hsm-internal.sh"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<tt>hsm-internal.sh</tt> is being called for each file of with <tt>osm=dcache</tt> with the command <tt>put</tt>. It is also used with the command <tt>get</tt> to retrieve the file again from the archive."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat ../../skel/usr/share/dcache/lib/hsm-internal.sh"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "pack-files.py"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The <tt>pack-files.py</tt> scripts collect the files the <tt>hsm-internal.sh</tt> creates entries for."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat ../../skel/usr/local/bin/pack-files.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "fillmetadata.py"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat ../../skel/usr/local/bin/fillmetadata.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "writebfids.py"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat ../../skel/usr/local/bin/writebfids.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}